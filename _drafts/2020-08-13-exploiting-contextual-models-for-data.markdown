---
title: Adapting Text Augmentation to Industry problems
excerpt: Applying recent NLP-Augmentation ideas to Industrial projects
tags: [Augmentation, NLP, Natural Language Processing, Data Augmentation]
date: 2020-08-24 12:38:10 +0530
categories: blogs nlp
permalink: /:categories/:title
gallery:
  - url: /assets/images/augmentationimg.PNG
    image_path: /assets/images/augmentationimg.PNG
    alt: "placeholder image 1 - freemiya vanilla contextual lang-model"
    title: "_Synthetic variants generated using BERT_"
  - url: /assets/images/redaction_roberta.PNG
    image_path: /assets/images/redaction_roberta.PNG
    alt: "placeholder image 2"
    title: "_Synthetic variants generated using RoBERTa_"

feature_row:
  - image_path: /assets/images/augmentationimg.PNG
    alt: "placeholder image 1 - freemiya vanilla contextual lang-model"
    title: "_Synthetic variants generated using BERT_"
    excerpt: "Generated using [freemiya's nlp-helper](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps). If mask is not provided, it will randomly mask the words. "
  - image_path: /assets/images/redaction_roberta.PNG
    alt: "placeholder image 2"
    title: "_Synthetic variants generated using RoBERTa_"
    excerpt: "This is useful when you're trying to augment data for your NER system."
    url: "#test-link"
    btn_label: "Read More"
    btn_class: "btn--inverse"

---



In this post, I will talk about the recent advances in exploiting language models for data generation and also show how, where we can implement them in Industry.

## Context

The problem with Data Augmentation in NLP is that language is position sensitive i.e any change in the word order will change its syntax thereby affecting its semantics. Despite the stringent constraints in language, Data Augmentation has picked up significant pace in the recent past.

Twitter NLP community has woken up with the paper [Easy Data Augmentation](https://arxiv.org/abs/1901.11196), followed by a popular [blog](https://amitness.com/2020/05/data-augmentation-for-nlp/) on data augmentation techniques. The basic ones include lexical substitution, robustness through spellings, etc. Exploiting-Language-Models is newly added to the catalogue. This post focusses exclusively on the latter-part.


## Use-case scenarios

For this post, I would like to consider two Industrial tasks where we can use augmentation:
1. __Categorizing complaints/survey-responses into specific categories.__ (_Classification task_)
    
    Let's say that you are a Computer Tech-Giant like Microsoft and want to process your incoming messages into clear buckets like Hardware, Customer Care, Software, General Support, Tech Support, Refund etc.

2. __Redacting sensitive information from the data.__ (_Sequence Labeling task_)

    Let's consider that you are an independent survey firm who's documenting the statistics of COVID-19. In this survey, to respect the privacy of individuals involved and to avoid targetted harassment by the public, few statements must be __stripped__ of the key details. _For example:_
> In Boston Street #4, Infected indivuals were two __Asian__ students, of age __22__ and __25__ years old.

## Leveraging Language models

This section will talk about the new techniques and also shows how they can be adapted to the use-cases above.

### 1. Vanilla method
    
This is as simple as asking BERT/RoBERTa etc to fill-in the `<mask>` entries. This way, we can generate synthetic variants of the given sentence.

__Info__: In the image below, the app will randomly mask the sentence (_if not masked already_) and predicts the `<mask>`.

{% include figure image_path="/assets/images/augmentationimg.PNG" alt="freemiya vanilla contextual lang-model" caption="__Figure 1:__ _Synthetic variants generated using BERT_. Reference: [freemiya repo](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps)" %}

#### 1.1 Applying to the use-case

This vanilla method can be applied directly to usecase #2 i.e `Redaction`. Let's consider a sentence for this purpose.

> The two participants in the Induction program were __white__ males, __6__ and __9__ years of age.

Here, we can generate multiple versions for race and age. Let's mask these occurences and make the Language-models predict them. These generated variants can address the entity-level imbalance issue. 

{% include figure image_path="/assets/images/redaction_roberta.PNG" alt="application of vanilla contextual lang-model" caption="__Figure 2:__ _Synthetic variants generated using RoBERTa_. Reference: [freemiya repo](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps)" %}

In the previous method, I __comfortably ignored__ the fact that there can be class shiftors as well i.e newly predicted words can change the category of the sentence. This is as good as noise.

This is handled in the following methods:

### 2. Pattern-Exploiting Training (PET):

PET is a euphemism for the paper [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/abs/2001.07676). In this paper, authors set their objective to assign soft-labels to the unlabeled data using language-models and thereby creating larger datasets for improved supervised learning.

To achieve soft-labels that are relevant to the task, authors suggest reformulating the input sentence to help the language model in identifying the target task.

Take an example,
> Awesome drink specials during happy hour.
Fantastic wings that are crispy and delicious,
wing night on Tuesday and Thursday!

In order to summarize the sentence by exploiting LMs, authors suggest us to introduce supporting phrases. For the above example, we can append _All in all, it was a `<mask>` experience_ and make the LMs predict the mask.

Example now:
> Awesome drink specials during happy hour.
Fantastic wings that are crispy and delicious,
wing night on Tuesday and Thursday! __All in all, it was a `<mask>` experience__


{% include figure image_path="/assets/images/only-words-pet_word_mask_prediction.png" alt="PET-Cloze using demo on yelp" caption="__Figure 3:__ _Synthetic variants generated using RoBERTa_. Reference: [Renato Voilin repo](https://github.com/renatoviolin/next_word_prediction)" %}


> Pizza was good but upset with price.

When you want to build a +ve/-ve classifier, these kind of sentences fall in the _gray_ area. The review can be counted as +ve if we are considering __taste__ and -ve if we are considering __price__.

Specifically, they add a masked phrase.

You add a pattern to the language models 

## Appendix

[TextAttack]() and [Checklist]() are officially using contextual models for generating data/test-cases. So, this approach should not be a surprise for you.

__Info__: Libraries like [nlpaug](https://github.com/makcedward/nlpaug), [eda_nlp](https://github.com/jasonwei20/eda_nlp) and [TextAttack](https://github.com/QData/TextAttack) are already offering most of these methods.