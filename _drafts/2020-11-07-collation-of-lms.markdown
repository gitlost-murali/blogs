---
title: Priming differently
excerpt: Discussing different pre-training techniques 
tags: [Language Models, LMs, NLP, Natural Language Processing, Pre-Training, Deep Learning]
date: 2020-11-07 21:00:10 +0530
categories: nlp lms
permalink: /:categories/:title
---


Current representation learning methods (like BERT, ALBERT, XLNet, etc.) can be viewed in the lens of Denoising Auto-Encoders. 

## ELECTRA

In the classic BERT-style pre-training, 
1. Corruption: We mask out some tokens in the sentence
2. Reconstruction: Make predictions on the masked tokens' representations. 

ELECTRA negates these two and proposes alternatives,
1. Corruption: Instead of masking the selected tokens, replace them with plausible alternatives generated by the generator (a smaller Masked LM).
2. Reconstruction: Make predictions on __all__ the tokens' representations to distinguish whether they are _original_ tokens or the _replaced_ ones.

Authors claim that ELECTRA method is more effective since it is defined over all input tokens rather than the subset that was masked out.

__Why?__

Models like BERT & its variants, although effective at learning bidirectional representations, are computationally expensive because they learn only from 15% of the tokens (masked-out tokens) per example.

Whereas in ELECTRA, 100% of the tokens are used (task-2: Distinguishing whether a token is an actual one or a replaced one).

In BERT, there is a major mismatch b/w the pre-training and finetuning phases as the artifical `[MASK]` token is seen while pre-training but not when finetuning on the downstream tasks.

## XLNet:

BERT majorly has 3 problems in the eyes of researchers:
1. Computational cost to learning ratio: For each sample, network is only learning from the 15% of the tokens.
2. Mismatch in input during pre-training and fine-tuning phases i.e [MASK] token only appears during pre-training.
3. Assumes that the predicted ([MASKED]) tokens are independent of each other i.e If 3rd and 8th tokens are masked, prediction of 8th doesn't depend on the output of 3rd and vice-versa.
    1. Example: _The movie is so [MASK] that I [MASK] watch it again._

    Plausible answers for 5th token is "good, wonderful" or "bad, worse".

    Plausible answers for 8th token is "will, may" or "won't, can't"

    It is entirely possible to get an incoherent sentence like "The movie is so bad that I will watch it again"

XLNet tries to address the 3rd drawback i.e incoherence among predictions through __Auto-Regressive Language Modeling__ powered by Permutation Language Modeling.

### Why Auto-Regressive Language Modeling?

An Auto-regressive language model follows a joint probability distribution implicitly i.e each predicted words depends on the previous words. Whereas for BERT, that's not the case.

Example: For the sentence "I am from the city, New York", let's say you want to predict [New, York] from the sequence. Then, the prediction loss would be,

For BERT,

P(New,York) = P(New \| I am from the city,) + P(York \| I am from the city,)

Whereas, for an AR-LM

P(New,York) = P(New \| I am from the city) + P(York \| I am from the city, New)

### Getting the best of both worlds

Auto-regressive can march only in one direction either forward or backward. This is where BERT shines in providing bi-directional context but fails at the aforementioned points like Pre-Train/Fine-Tune discrepancy/Independent predictions.

Authors propose to integrate bidirectional context into Auto-Regressive lang models since that's the one piece missing in it and stood-out for BERT.

### Integrate bidirectional-context through Permutation Language Modeling:

Authors propose to generate permutations of the sequence for providing bidirectional context. Following are the steps:

Permutations of the sequence x1,x2,x3,x4 are:
```
x1, x2, x3, x4
x2, x1, x3, x4
x2, x3, x1, x4
x2, x4, x3, x1
x4, x2, x3, x1
...
```

These permutations allow the model to look at the future words. Ex: While predicting x3, model can now look at the future words like x4.


__Note:__ Here, sequence of words is not changed but only the order of prediction. Order of prediction is not necessarily left to right and is sampled randomly instead. For instance, it could be

__Order of prediction in traditional AR-LM:__ 

"I", "like", "cats", "more", "than", "dogs"

__Order of prediction in Permutated LM:__ 

"cats", "than", "I", "more", "dogs", "like"

![Permutation example](/assets/images/permutation-example.gif)

An example of how a permutation language model would predict tokens for a certain permutation. Unfaded words are provided as input to the model while faded words are masked out.

## References:

1. https://towardsdatascience.com/what-is-xlnet-and-why-it-outperforms-bert-8d8fce710335

2. https://arxiv.org/pdf/2003.08271.pdf

3. https://arxiv.org/pdf/2010.00854.pdf

4. https://mlexplained.com/2019/06/30/paper-dissected-xlnet-generalized-autoregressive-pretraining-for-language-understanding-explained/