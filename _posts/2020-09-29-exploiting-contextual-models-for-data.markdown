---
title: Adapting Text Augmentation to Industry problems
excerpt: Applying recent NLP-Augmentation ideas to Industrial projects
tags: [Augmentation, NLP, Natural Language Processing, Data Augmentation]
date: 2020-09-29 19:58:10 +0530
categories: blogs nlp
tags: [Data-Augmentation, Language Models, LMs, Machine Learning, Deep Learning, NLP]
permalink: /:categories/:title
gallery:
  - url: /assets/images/augmentationimg.PNG
    image_path: /assets/images/augmentationimg.PNG
    alt: "placeholder image 1 - freemiya vanilla contextual lang-model"
    title: "_Synthetic variants generated using BERT_"
  - url: /assets/images/redaction_roberta.PNG
    image_path: /assets/images/redaction_roberta.PNG
    alt: "placeholder image 2"
    title: "_Synthetic variants generated using RoBERTa_"

feature_row:
  - image_path: /assets/images/augmentationimg.PNG
    alt: "placeholder image 1 - freemiya vanilla contextual lang-model"
    title: "_Synthetic variants generated using BERT_"
    excerpt: "Generated using [freemiya's nlp-helper](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps). If mask is not provided, it will randomly mask the words. "
  - image_path: /assets/images/redaction_roberta.PNG
    alt: "placeholder image 2"
    title: "_Synthetic variants generated using RoBERTa_"
    excerpt: "This is useful when you're trying to augment data for your NER system."
    url: "#test-link"
    btn_label: "Read More"
    btn_class: "btn--inverse"

---



In this post, I will talk about the recent advances in exploiting language models for data generation and also show how, where we can implement them in Industry.

## Context

The problem with Data Augmentation in NLP is that _language is position sensitive_ i.e any change in the word order will change its syntax thereby affecting its semantics. Despite the stringent constraints in language, Data Augmentation has picked up significant pace in the recent past.

Twitter NLP community has woken up with the paper [Easy Data Augmentation](https://arxiv.org/abs/1901.11196), followed by a popular [blog](https://amitness.com/2020/05/data-augmentation-for-nlp/) on data augmentation techniques. The basic ones include lexical substitution, robustness through spellings, etc. Exploiting-Language-Models is newly added to the catalogue. This post focusses exclusively on the latter-part.


## Use-case scenarios

For this post, I would like to consider two Industrial tasks where we can use augmentation:
1. __Categorizing complaints/survey-responses into specific categories.__ (_Classification task_)
    
    Let's say that you are a Computer Tech-Giant like HP/Microsoft and want to process your incoming messages into clear buckets like Hardware, Customer Care, Software, General Support, Tech Support, Refund etc.

2. __Redacting sensitive information from the data.__ (_Sequence Labeling task_)

    Let's consider that you are an independent survey firm who's documenting the statistics of COVID-19. In this survey, to respect the privacy of individuals involved and to avoid targetted harassment by the public, few statements must be __stripped__ of the key details. _For example:_
> In Boston Street #4, Infected indivuals were two __Asian__ students, of age __22__ and __25__ years old.

## Leveraging Language models

This section will talk about the new techniques and also shows how they can be adapted to the use-cases above.

### 1. Vanilla method
    
This is as simple as asking BERT/RoBERTa etc to fill-in the `<mask>` entries. This way, we can generate synthetic variants of the given sentence.

__Info__: In the image below, the app will mask random words (_if not masked already_) and predicts the `<mask>`.

{% include figure image_path="/assets/images/augmentationimg.PNG" alt="freemiya vanilla contextual lang-model" caption="__Figure 1:__ _Synthetic variants generated using BERT_. Reference: [freemiya repo](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps)" %}

#### 1.1 Applying to the use-case

This vanilla method can be applied directly to usecase #2 i.e `Redaction`. Let's consider a sentence for this purpose.

> The two participants in the Induction program were __white__ males, __6__ and __9__ years of age.

Here, we can generate multiple versions for race and age. Let's mask these occurences and make the Language-models predict them. These generated variants can address the entity-level imbalance issue. 

{% include figure image_path="/assets/images/redaction_roberta.PNG" alt="application of vanilla contextual lang-model" caption="__Figure 2:__ _Synthetic variants generated using RoBERTa_. Reference: [freemiya repo](https://github.com/freemiya/nlp-helper/tree/master/augmentation_apps)" %}

In the previous method, I __comfortably ignored__ the fact that there can be __class shiftors__ as well i.e newly predicted words __can change the category__ of the sentence. For example, we want __"white"__ word to be replaced by only "races" like Caucasian, Hispanic but we can see that there are other words which doesn't necessarily represent "race". This is as good as noise.

This is handled in [Section 3.1](http://localhost:4000/blogs/nlp/exploiting-contextual-models-for-data#3-conditioning-your-input).

### 2. Pattern-Exploiting Training (PET):

#### 2.1 Intro:

PET is a euphemism for the paper [Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference](https://arxiv.org/abs/2001.07676). In this paper, authors set their objective to __assign soft-labels__ to the unlabeled data using language-models and thereby creating larger datasets for improved supervised learning.

To achieve soft-labels that are relevant to the task, authors suggest reformulating the input sentence to help the language model in identifying the target task.

For example, let's take a review:
> Awesome drink specials during happy hour.
Fantastic wings that are crispy and delicious,
wing night on Tuesday and Thursday!

In order to tag the above sentence by exploiting LMs, authors suggest introducing supporting phrases/templates. For the above example, we can use
> __All in all, it was a `<mask>` experience__

as a supporting phrase and make the LMs predict the mask. After appending this template sentence to our review, let us review what does the LMs predict now:
> Awesome drink specials during happy hour.
Fantastic wings that are crispy and delicious,
wing night on Tuesday and Thursday! __All in all, it was a `<mask>` experience__


{% include figure image_path="/assets/images/only-words-pet_word_mask_prediction.png" alt="PET-Cloze using demo on yelp" caption="__Figure 3:__ _Synthetic variants generated using RoBERTa_. Reference: [Renato Voilin repo](https://github.com/renatoviolin/next_word_prediction)" %}

We can observe that words like _wonderful, great, fantastic_ in Figure 3 are suggesting that the review is a __+ve one__.

#### 2.2 Narrowing down the search space

Let's consider another review,

> Pizza was good but disappointed with the ambiance.

When you want to build a +ve/-ve classifier, these kind of sentences fall in the __gray__ area. The review can be counted as +ve if we are considering __Taste__ and -ve if we are considering __Ambiance__. This is where you need to put in your human-bias as to let the models know what you want. __Ambiance/Taste__.

{% include figure image_path="/assets/images/dichotomy-pet-annot.png" alt="PET-Cloze using demo on yelp" caption="__Figure 4:__ _Responses generated by various LMs_. Reference: [Renato Voilin repo](https://github.com/renatoviolin/next_word_prediction)" %}

If __Taste__ is the criterion you want to consider, then predicted words like _bad, disappointing_ (red) need to replaced with positive words (green). So, you can fine-tune your LMs to predict the words of your choice. More details like, how mapping multiple words to a class-label is done, is out of scope for this post. Finetuning to words of your choice will help us narrow down the search-space of newly predicted words to that of the domain.

This method can be applied to Usecase #1 i.e `Categorizing complaints`

{% include figure image_path="/assets/images/pet-classify.png" alt="PET-Cloze on classification" caption="__Figure 5:__ You can finetune your LMs to the output terms like Research team or dev team. Reference: [Renato Voilin repo](https://github.com/renatoviolin/next_word_prediction)" %}

### 3. Conditioning your input:  

Another way to handle __class shiftors__ is to tell your LMs what you want extrinsically. [Conditional-BERT](https://www.iccs-meeting.org/archive/iccs2019/papers/115390083.pdf) authors approached this by prepending the desired class-label as an input.

{% include figure image_path="/assets/images/conditional-BERT.png" alt="C-BERT" caption="__Figure 6:__ _Conditional BERT for Data Augmentation_. Reference: [C-BERT paper](https://www.iccs-meeting.org/archive/iccs2019/papers/115390083.pdf)" %}

[Varun Kumar et.al](https://arxiv.org/pdf/2003.02245.pdf) improved this method by including Auto-Regressive Language-Models like GPT-2 & Seq-Seq Language Models like BART. 

Nevertheless, conditional-input method must be changed slightly to adapt to the __Use-case #2__ i.e __Redaction/NER__. If you want a particular masked word to be "race", add "race" at its position in the label-embeddings-layer to your input. Same goes for other entities like age, country etc.

## Conclusion

In this post, we have gone through the Papers like PET, C-BERT and others to discuss the latest methods used in exploiting language models for Data-Augmentation. We also discussed how we can adapt these to Industry problems.

## Appendix

Libraries like [nlpaug](https://github.com/makcedward/nlpaug), [eda_nlp](https://github.com/jasonwei20/eda_nlp), [TextAttack](https://github.com/QData/TextAttack) are already offering most of these methods and [Checklist](https://github.com/marcotcr/checklist), [TextAttack](https://github.com/QData/TextAttack) are officially using contextual models for generating test-cases/adverse-attacks.

## References:

- Varun Kumar et.al -> [Data Augmentation using Pre-trained Transformer Models](https://arxiv.org/pdf/2003.02245.pdf)
- Xing Wu et.al -> [Conditional BERT Contextual Augmentation](https://www.iccs-meeting.org/archive/iccs2019/papers/115390083.pdf)
- Jason Wei, Kai Zou -> [Easy Data Augmentation](https://arxiv.org/abs/1901.11196)
- Marco Tulio Ribeiro et.al -> Checklist [Paper](http://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf) and [repo](https://github.com/marcotcr/checklist)